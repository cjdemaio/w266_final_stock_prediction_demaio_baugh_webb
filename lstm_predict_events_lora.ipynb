{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockDataset(Dataset):\n",
    "    def __init__(self, df, seq_length=60):\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "        tickers = df['Ticker'].unique()\n",
    "\n",
    "        for ticker in tickers:\n",
    "            stock_data = df[df['Ticker'] == ticker].reset_index(drop=True)\n",
    "\n",
    "            # Ensure no missing values\n",
    "            stock_data.fillna(0, inplace=True)\n",
    "\n",
    "            # Select features: Add recommended features\n",
    "            features = stock_data[\n",
    "                [\n",
    "                    'Open', 'High', 'Low', 'Close', 'Volume',  # Basic features\n",
    "                    'RSI', 'MACD', 'Signal_Line', 'Bollinger_Width',  # Technical indicators\n",
    "                    'Day_of_Week', 'Month',  # Time-based features\n",
    "                    'Lag1_Open', 'Lag2_Open', 'Lag1_Volume',\n",
    "                    'Average_sentiment', 'news_day',  # Lagged features\n",
    "                    'I-A', 'I-CT', 'I-RD', 'I-DC', 'I-DI', 'I-GI', 'I-NC',\n",
    "                    'I-RSS', 'I-SD', 'I-SR', 'I-SS', 'O', 'I-GC' # Business Event Indicator variables\n",
    "                ]\n",
    "            ].values\n",
    "\n",
    "            # Extract the target: Movement (binary)\n",
    "            movements = stock_data['Movement'].values\n",
    "\n",
    "            # Create sliding windows for sequences\n",
    "            for i in range(len(features) - seq_length):\n",
    "                self.data.append(features[i:i + seq_length])  # Sequence of features\n",
    "                self.labels.append(movements[i + seq_length])  # Target for next day\n",
    "\n",
    "        # Convert to numpy arrays for better handling\n",
    "        self.data = np.array(self.data)\n",
    "        self.labels = np.array(self.labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return sequence data and label as tensors\n",
    "        return (\n",
    "            torch.tensor(self.data[idx], dtype=torch.float32),\n",
    "            torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM model for binary classification\n",
    "class StockLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(StockLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=0.2)\n",
    "        self.fc = nn.Linear(hidden_size, 1)  # Single output for binary classification\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h_0 = torch.zeros(num_layers, x.size(0), hidden_size).to(x.device)\n",
    "        c_0 = torch.zeros(num_layers, x.size(0), hidden_size).to(x.device)\n",
    "        out, _ = self.lstm(x, (h_0, c_0))  # LSTM output\n",
    "        hidden_state = out[:, -1, :]  # Use the last hidden state\n",
    "        return self.fc(hidden_state)  # Return raw logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, device, patience=5):\n",
    "    best_val_loss = float('inf')\n",
    "    no_improve_epochs = 0  # Tracks epochs without improvement\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device).unsqueeze(1).float()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device).unsqueeze(1).float()\n",
    "                outputs = model(inputs)\n",
    "                val_loss += criterion(outputs, targets).item()\n",
    "                \n",
    "                # Compute accuracy\n",
    "                predictions = (torch.sigmoid(outputs) > 0.5).float()\n",
    "                correct_predictions += (predictions == targets).sum().item()\n",
    "                total_predictions += targets.size(0)\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        accuracy = correct_predictions / total_predictions\n",
    "\n",
    "        # Print progress\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "        # Scheduler step\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        # Check for improvement\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            no_improve_epochs = 0\n",
    "            torch.save(model.state_dict(), 'best_model.pth')  # Save the best model\n",
    "        else:\n",
    "            no_improve_epochs += 1\n",
    "\n",
    "        # Early stopping\n",
    "        if no_improve_epochs >= patience:\n",
    "            print(\"Early stopping triggered. Stopping training.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear) or isinstance(m, nn.LSTM):\n",
    "        for name, param in m.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.xavier_uniform_(param.data)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.zeros_(param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in data_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            predictions = (torch.sigmoid(outputs) > 0.5).float()\n",
    "            \n",
    "            y_true.extend(targets.cpu().numpy())\n",
    "            y_pred.extend(predictions.cpu().numpy())\n",
    "    \n",
    "    # Print classification report\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    \n",
    "    # Print confusion matrix\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_size = 29  # Number of input features (OHLCV, sentiment, news_day, event indicators)\n",
    "hidden_size = 96\n",
    "num_layers = 2\n",
    "seq_length = 60\n",
    "batch_size = 64\n",
    "num_epochs = 20\n",
    "learning_rate = 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation\n",
    "df = pd.read_csv('sp100_ohlcv_news_sentiment_events_lora.csv')  # Load your dataframe\n",
    "\n",
    "# Calculate Movement: 1 if next day's Open is greater, 0 otherwise\n",
    "df['Movement'] = (df['Open'].shift(-1) > df['Open']).astype(int)\n",
    "df = df.iloc[:-1]  # Drop the last row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate 5-day EMA\n",
    "def calculate_ema(series, span=5):\n",
    "    return series.ewm(span=span, adjust=False).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply 5-day EMA to Average_sentiment\n",
    "df['Sentiment_EMA'] = calculate_ema(df['Average_sentiment'])\n",
    "\n",
    "# Fill missing Average_sentiment values (where news_day == 0) with the EMA\n",
    "df.loc[df['news_day'] == 0, 'Average_sentiment'] = df.loc[df['news_day'] == 0, 'Sentiment_EMA']\n",
    "\n",
    "# Drop the temporary Sentiment_EMA column\n",
    "df.drop(columns=['Sentiment_EMA'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Ticker</th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Average_sentiment</th>\n",
       "      <th>news_day</th>\n",
       "      <th>...</th>\n",
       "      <th>I-DI</th>\n",
       "      <th>I-GI</th>\n",
       "      <th>I-NC</th>\n",
       "      <th>I-RSS</th>\n",
       "      <th>I-SD</th>\n",
       "      <th>I-SR</th>\n",
       "      <th>I-SS</th>\n",
       "      <th>O</th>\n",
       "      <th>I-GC</th>\n",
       "      <th>Movement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>396</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-06-02</td>\n",
       "      <td>147.83</td>\n",
       "      <td>151.27</td>\n",
       "      <td>146.86</td>\n",
       "      <td>151.21</td>\n",
       "      <td>72348055</td>\n",
       "      <td>-3.328278e-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>397</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-06-01</td>\n",
       "      <td>149.90</td>\n",
       "      <td>151.74</td>\n",
       "      <td>147.68</td>\n",
       "      <td>148.71</td>\n",
       "      <td>74286635</td>\n",
       "      <td>-3.328278e-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>398</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-05-31</td>\n",
       "      <td>149.07</td>\n",
       "      <td>150.66</td>\n",
       "      <td>146.84</td>\n",
       "      <td>148.84</td>\n",
       "      <td>103718416</td>\n",
       "      <td>-3.328278e-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>399</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-05-27</td>\n",
       "      <td>145.39</td>\n",
       "      <td>149.68</td>\n",
       "      <td>145.26</td>\n",
       "      <td>149.64</td>\n",
       "      <td>90978503</td>\n",
       "      <td>-3.328278e-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>400</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-05-26</td>\n",
       "      <td>137.39</td>\n",
       "      <td>144.34</td>\n",
       "      <td>137.14</td>\n",
       "      <td>143.78</td>\n",
       "      <td>90601548</td>\n",
       "      <td>-3.328278e-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151911</th>\n",
       "      <td>151911</td>\n",
       "      <td>XOM</td>\n",
       "      <td>2018-01-09</td>\n",
       "      <td>86.94</td>\n",
       "      <td>87.15</td>\n",
       "      <td>86.65</td>\n",
       "      <td>86.77</td>\n",
       "      <td>7870756</td>\n",
       "      <td>-2.898843e-40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151912</th>\n",
       "      <td>151912</td>\n",
       "      <td>XOM</td>\n",
       "      <td>2018-01-08</td>\n",
       "      <td>86.70</td>\n",
       "      <td>87.15</td>\n",
       "      <td>86.60</td>\n",
       "      <td>87.14</td>\n",
       "      <td>10549116</td>\n",
       "      <td>-2.898843e-40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151913</th>\n",
       "      <td>151913</td>\n",
       "      <td>XOM</td>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>86.75</td>\n",
       "      <td>86.88</td>\n",
       "      <td>85.71</td>\n",
       "      <td>86.75</td>\n",
       "      <td>11003133</td>\n",
       "      <td>-2.898843e-40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151914</th>\n",
       "      <td>151914</td>\n",
       "      <td>XOM</td>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>86.79</td>\n",
       "      <td>87.22</td>\n",
       "      <td>86.43</td>\n",
       "      <td>86.82</td>\n",
       "      <td>10840055</td>\n",
       "      <td>-2.898843e-40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151915</th>\n",
       "      <td>151915</td>\n",
       "      <td>XOM</td>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>85.16</td>\n",
       "      <td>86.97</td>\n",
       "      <td>84.82</td>\n",
       "      <td>86.70</td>\n",
       "      <td>13862035</td>\n",
       "      <td>-2.898843e-40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>59860 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0 Ticker        Date    Open    High     Low   Close  \\\n",
       "396            396   AAPL  2022-06-02  147.83  151.27  146.86  151.21   \n",
       "397            397   AAPL  2022-06-01  149.90  151.74  147.68  148.71   \n",
       "398            398   AAPL  2022-05-31  149.07  150.66  146.84  148.84   \n",
       "399            399   AAPL  2022-05-27  145.39  149.68  145.26  149.64   \n",
       "400            400   AAPL  2022-05-26  137.39  144.34  137.14  143.78   \n",
       "...            ...    ...         ...     ...     ...     ...     ...   \n",
       "151911      151911    XOM  2018-01-09   86.94   87.15   86.65   86.77   \n",
       "151912      151912    XOM  2018-01-08   86.70   87.15   86.60   87.14   \n",
       "151913      151913    XOM  2018-01-05   86.75   86.88   85.71   86.75   \n",
       "151914      151914    XOM  2018-01-04   86.79   87.22   86.43   86.82   \n",
       "151915      151915    XOM  2018-01-03   85.16   86.97   84.82   86.70   \n",
       "\n",
       "           Volume  Average_sentiment  news_day  ...  I-DI  I-GI  I-NC  I-RSS  \\\n",
       "396      72348055      -3.328278e-01       0.0  ...   0.0   0.0   0.0    0.0   \n",
       "397      74286635      -3.328278e-01       0.0  ...   0.0   0.0   0.0    0.0   \n",
       "398     103718416      -3.328278e-01       0.0  ...   0.0   0.0   0.0    0.0   \n",
       "399      90978503      -3.328278e-01       0.0  ...   0.0   0.0   0.0    0.0   \n",
       "400      90601548      -3.328278e-01       0.0  ...   0.0   0.0   0.0    0.0   \n",
       "...           ...                ...       ...  ...   ...   ...   ...    ...   \n",
       "151911    7870756      -2.898843e-40       0.0  ...   0.0   0.0   0.0    0.0   \n",
       "151912   10549116      -2.898843e-40       0.0  ...   0.0   0.0   0.0    0.0   \n",
       "151913   11003133      -2.898843e-40       0.0  ...   0.0   0.0   0.0    0.0   \n",
       "151914   10840055      -2.898843e-40       0.0  ...   0.0   0.0   0.0    0.0   \n",
       "151915   13862035      -2.898843e-40       0.0  ...   0.0   0.0   0.0    0.0   \n",
       "\n",
       "        I-SD  I-SR  I-SS    O  I-GC  Movement  \n",
       "396      0.0   0.0   0.0  0.0   0.0         1  \n",
       "397      0.0   0.0   0.0  0.0   0.0         0  \n",
       "398      0.0   0.0   0.0  0.0   0.0         0  \n",
       "399      0.0   0.0   0.0  0.0   0.0         0  \n",
       "400      0.0   0.0   0.0  0.0   0.0         1  \n",
       "...      ...   ...   ...  ...   ...       ...  \n",
       "151911   0.0   0.0   0.0  0.0   0.0         0  \n",
       "151912   0.0   0.0   0.0  0.0   0.0         1  \n",
       "151913   0.0   0.0   0.0  0.0   0.0         1  \n",
       "151914   0.0   0.0   0.0  0.0   0.0         0  \n",
       "151915   0.0   0.0   0.0  0.0   0.0         0  \n",
       "\n",
       "[59860 rows x 24 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[(df['news_day'] == 0) & df['Average_sentiment'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rsi(data, window=14):\n",
    "    delta = data['Close'].diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()\n",
    "    rs = gain / loss\n",
    "    return 100 - (100 / (1 + rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chris\\AppData\\Local\\Temp\\ipykernel_53128\\4135909989.py:2: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['RSI'].fillna(50, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "df['RSI'] = calculate_rsi(df)\n",
    "df['RSI'].fillna(50, inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['EMA_12'] = df['Close'].ewm(span=12, adjust=False).mean()\n",
    "df['EMA_26'] = df['Close'].ewm(span=26, adjust=False).mean()\n",
    "df['MACD'] = df['EMA_12'] - df['EMA_26']\n",
    "df['Signal_Line'] = df['MACD'].ewm(span=9, adjust=False).mean()\n",
    "df.drop(columns=['EMA_12', 'EMA_26'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bollinger_bands(data, window=20, k=2):\n",
    "    ma = data['Close'].rolling(window=window).mean()\n",
    "    std = data['Close'].rolling(window=window).std()\n",
    "    data['Upper_Band'] = ma + k * std\n",
    "    data['Lower_Band'] = ma - k * std\n",
    "    data['Bollinger_Width'] = data['Upper_Band'] - data['Lower_Band']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_bollinger_bands(df)\n",
    "df.drop(columns=['Upper_Band', 'Lower_Band'], inplace=True)  # Keep only Bollinger_Width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal features\n",
    "df['Day_of_Week'] = pd.to_datetime(df['Date']).dt.dayofweek\n",
    "df['Month'] = pd.to_datetime(df['Date']).dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lagged features\n",
    "df['Lag1_Open'] = df['Open'].shift(1)\n",
    "df['Lag2_Open'] = df['Open'].shift(2)\n",
    "df['Lag1_Volume'] = df['Volume'].shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 RSI           MACD      Lag1_Open      Lag2_Open  \\\n",
      "count  151916.000000  151916.000000  151915.000000  151914.000000   \n",
      "mean       47.737625      -0.004999     218.758783     218.759652   \n",
      "std        17.400732      19.269516     370.899669     370.900736   \n",
      "min         0.000000    -308.825915       4.270000       4.270000   \n",
      "25%        35.130930      -1.749317      64.530000      64.530000   \n",
      "50%        47.540525      -0.242395     121.540000     121.540000   \n",
      "75%        59.916194       1.001487     220.770000     220.770000   \n",
      "max       100.000000     959.923798    3744.000000    3744.000000   \n",
      "\n",
      "        Lag1_Volume  \n",
      "count  1.519150e+05  \n",
      "mean   1.127710e+07  \n",
      "std    1.833312e+07  \n",
      "min    9.734500e+04  \n",
      "25%    2.770560e+06  \n",
      "50%    5.284210e+06  \n",
      "75%    1.107402e+07  \n",
      "max    4.010487e+08  \n"
     ]
    }
   ],
   "source": [
    "print(df[['RSI', 'MACD', 'Lag1_Open', 'Lag2_Open', 'Lag1_Volume']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize numerical features to improve learning\n",
    "scaler = MinMaxScaler()\n",
    "# Select numerical features to normalize (include new features)\n",
    "numerical_features = ['Open', 'High', 'Low', 'Close', 'Volume', 'RSI', 'MACD', 'Signal_Line', 'Bollinger_Width', 'Lag1_Open', 'Lag2_Open', 'Lag1_Volume']\n",
    "df[numerical_features] = scaler.fit_transform(df[numerical_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df[df['Date'] < '2023-01-01']\n",
    "test_df = df[df['Date'] >= '2023-01-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights: tensor([0.9550, 1.0494], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(train_df['Movement']),\n",
    "    y=train_df['Movement']\n",
    ")\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "print(\"Class Weights:\", class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train_df into train and validation sets\n",
    "train_split = int(len(train_df) * 0.9)\n",
    "train_data = train_df.iloc[:train_split]\n",
    "val_data = train_df.iloc[train_split:]\n",
    "\n",
    "train_dataset = StockDataset(train_data, seq_length)\n",
    "val_dataset = StockDataset(val_data, seq_length)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Chris\\anaconda3\\envs\\p310\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model, Loss, Optimizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = StockLSTM(input_size, hidden_size, num_layers).to(device)\n",
    "model.apply(init_weights)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=class_weights[1])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', patience=3, factor=0.1, verbose=True\n",
    ")\n",
    "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model on 2018-2022 data...\n",
      "Epoch [1/100], Train Loss: 0.5827, Val Loss: 0.5853, Val Accuracy: 0.6965\n",
      "Epoch [2/100], Train Loss: 0.5818, Val Loss: 0.5858, Val Accuracy: 0.6968\n",
      "Epoch [3/100], Train Loss: 0.5821, Val Loss: 0.5856, Val Accuracy: 0.6973\n",
      "Epoch [4/100], Train Loss: 0.5813, Val Loss: 0.5849, Val Accuracy: 0.6969\n",
      "Epoch [5/100], Train Loss: 0.5814, Val Loss: 0.5849, Val Accuracy: 0.6963\n",
      "Epoch [6/100], Train Loss: 0.5807, Val Loss: 0.5848, Val Accuracy: 0.6982\n",
      "Epoch [7/100], Train Loss: 0.5813, Val Loss: 0.5847, Val Accuracy: 0.6978\n",
      "Epoch [8/100], Train Loss: 0.5800, Val Loss: 0.5844, Val Accuracy: 0.6987\n",
      "Epoch [9/100], Train Loss: 0.5803, Val Loss: 0.5835, Val Accuracy: 0.7023\n",
      "Epoch [10/100], Train Loss: 0.5795, Val Loss: 0.5840, Val Accuracy: 0.7005\n",
      "Epoch [11/100], Train Loss: 0.5786, Val Loss: 0.5835, Val Accuracy: 0.6985\n",
      "Epoch [12/100], Train Loss: 0.5791, Val Loss: 0.5831, Val Accuracy: 0.7009\n",
      "Epoch [13/100], Train Loss: 0.5780, Val Loss: 0.5828, Val Accuracy: 0.6993\n",
      "Epoch [14/100], Train Loss: 0.5780, Val Loss: 0.5832, Val Accuracy: 0.7016\n",
      "Epoch [15/100], Train Loss: 0.5785, Val Loss: 0.5829, Val Accuracy: 0.7005\n",
      "Epoch [16/100], Train Loss: 0.5777, Val Loss: 0.5830, Val Accuracy: 0.7017\n",
      "Epoch [17/100], Train Loss: 0.5774, Val Loss: 0.5817, Val Accuracy: 0.7012\n",
      "Epoch [18/100], Train Loss: 0.5767, Val Loss: 0.5824, Val Accuracy: 0.7011\n",
      "Epoch [19/100], Train Loss: 0.5757, Val Loss: 0.5817, Val Accuracy: 0.7027\n",
      "Epoch [20/100], Train Loss: 0.5751, Val Loss: 0.5826, Val Accuracy: 0.7015\n",
      "Epoch [21/100], Train Loss: 0.5754, Val Loss: 0.5815, Val Accuracy: 0.7005\n",
      "Epoch [22/100], Train Loss: 0.5753, Val Loss: 0.5814, Val Accuracy: 0.7024\n",
      "Epoch [23/100], Train Loss: 0.5749, Val Loss: 0.5819, Val Accuracy: 0.7008\n",
      "Epoch [24/100], Train Loss: 0.5738, Val Loss: 0.5808, Val Accuracy: 0.7004\n",
      "Epoch [25/100], Train Loss: 0.5740, Val Loss: 0.5806, Val Accuracy: 0.7044\n",
      "Epoch [26/100], Train Loss: 0.5740, Val Loss: 0.5813, Val Accuracy: 0.7020\n",
      "Epoch [27/100], Train Loss: 0.5731, Val Loss: 0.5812, Val Accuracy: 0.6990\n",
      "Epoch [28/100], Train Loss: 0.5734, Val Loss: 0.5800, Val Accuracy: 0.7020\n",
      "Epoch [29/100], Train Loss: 0.5729, Val Loss: 0.5805, Val Accuracy: 0.7023\n",
      "Epoch [30/100], Train Loss: 0.5723, Val Loss: 0.5806, Val Accuracy: 0.7014\n",
      "Epoch [31/100], Train Loss: 0.5715, Val Loss: 0.5813, Val Accuracy: 0.7006\n",
      "Epoch [32/100], Train Loss: 0.5721, Val Loss: 0.5806, Val Accuracy: 0.7019\n",
      "Epoch [33/100], Train Loss: 0.5686, Val Loss: 0.5793, Val Accuracy: 0.7028\n",
      "Epoch [34/100], Train Loss: 0.5684, Val Loss: 0.5793, Val Accuracy: 0.7025\n",
      "Epoch [35/100], Train Loss: 0.5682, Val Loss: 0.5792, Val Accuracy: 0.7030\n",
      "Epoch [36/100], Train Loss: 0.5683, Val Loss: 0.5791, Val Accuracy: 0.7030\n",
      "Epoch [37/100], Train Loss: 0.5680, Val Loss: 0.5791, Val Accuracy: 0.7031\n",
      "Epoch [38/100], Train Loss: 0.5675, Val Loss: 0.5790, Val Accuracy: 0.7035\n",
      "Epoch [39/100], Train Loss: 0.5679, Val Loss: 0.5790, Val Accuracy: 0.7025\n",
      "Epoch [40/100], Train Loss: 0.5673, Val Loss: 0.5790, Val Accuracy: 0.7018\n",
      "Epoch [41/100], Train Loss: 0.5681, Val Loss: 0.5789, Val Accuracy: 0.7022\n",
      "Epoch [42/100], Train Loss: 0.5673, Val Loss: 0.5791, Val Accuracy: 0.7012\n",
      "Epoch [43/100], Train Loss: 0.5675, Val Loss: 0.5790, Val Accuracy: 0.7020\n",
      "Epoch [44/100], Train Loss: 0.5664, Val Loss: 0.5789, Val Accuracy: 0.7015\n",
      "Epoch [45/100], Train Loss: 0.5673, Val Loss: 0.5787, Val Accuracy: 0.7026\n",
      "Epoch [46/100], Train Loss: 0.5671, Val Loss: 0.5789, Val Accuracy: 0.7029\n",
      "Epoch [47/100], Train Loss: 0.5667, Val Loss: 0.5789, Val Accuracy: 0.7013\n",
      "Epoch [48/100], Train Loss: 0.5665, Val Loss: 0.5790, Val Accuracy: 0.7024\n",
      "Epoch [49/100], Train Loss: 0.5664, Val Loss: 0.5787, Val Accuracy: 0.7024\n",
      "Epoch [50/100], Train Loss: 0.5668, Val Loss: 0.5789, Val Accuracy: 0.7030\n",
      "Epoch [51/100], Train Loss: 0.5667, Val Loss: 0.5791, Val Accuracy: 0.7027\n",
      "Epoch [52/100], Train Loss: 0.5666, Val Loss: 0.5786, Val Accuracy: 0.7030\n",
      "Epoch [53/100], Train Loss: 0.5669, Val Loss: 0.5788, Val Accuracy: 0.7021\n",
      "Epoch [54/100], Train Loss: 0.5657, Val Loss: 0.5788, Val Accuracy: 0.7020\n",
      "Epoch [55/100], Train Loss: 0.5655, Val Loss: 0.5787, Val Accuracy: 0.7021\n",
      "Epoch [56/100], Train Loss: 0.5660, Val Loss: 0.5787, Val Accuracy: 0.7024\n",
      "Epoch [57/100], Train Loss: 0.5664, Val Loss: 0.5787, Val Accuracy: 0.7025\n",
      "Early stopping triggered. Stopping training.\n"
     ]
    }
   ],
   "source": [
    "# Initial training on 2018-2022 data\n",
    "print(\"Training the model on 2018-2022 data...\")\n",
    "train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    num_epochs=100,\n",
    "    device=device\n",
    ")\n",
    "torch.save(model.state_dict(), 'trained_model_events_lora.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.73      0.72      6296\n",
      "           1       0.69      0.67      0.68      5711\n",
      "\n",
      "    accuracy                           0.70     12007\n",
      "   macro avg       0.70      0.70      0.70     12007\n",
      "weighted avg       0.70      0.70      0.70     12007\n",
      "\n",
      "Confusion Matrix:\n",
      "[[4584 1712]\n",
      " [1860 3851]]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on validation/test set\n",
    "evaluate_model(model, val_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StockLSTM(\n",
       "  (lstm): LSTM(29, 96, num_layers=2, batch_first=True, dropout=0.2)\n",
       "  (fc): Linear(in_features=96, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = StockLSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers)\n",
    "model.load_state_dict(torch.load('trained_model_events_lora.pth'))\n",
    "model.to(device)\n",
    "model.eval()  # Set model to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_window_predictions_with_finetuning(\n",
    "    model, test_df, seq_length, device, optimizer, criterion, fine_tune_steps=1\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform rolling window predictions with dynamic model weight updates.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Trained LSTM model.\n",
    "        test_df (pd.DataFrame): DataFrame containing test data.\n",
    "        seq_length (int): Length of the input sequence for LSTM.\n",
    "        device (torch.device): Device (CPU/GPU) for model computation.\n",
    "        optimizer (torch.optim.Optimizer): Optimizer for fine-tuning the model.\n",
    "        criterion (torch.nn.Module): Loss function for fine-tuning.\n",
    "        fine_tune_steps (int): Number of gradient steps per update.\n",
    "\n",
    "    Returns:\n",
    "        predictions (list): List of predicted probabilities for each day.\n",
    "        actuals (list): List of actual Movement values.\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "\n",
    "    model.train()  # Switch to training mode for fine-tuning\n",
    "\n",
    "    for i in range(len(test_df) - seq_length - 1):\n",
    "        # Extract the rolling window\n",
    "        window_data = test_df.iloc[i:i + seq_length]\n",
    "\n",
    "        # Select features for the rolling window\n",
    "        features = window_data[\n",
    "                [\n",
    "                    'Open', 'High', 'Low', 'Close', 'Volume',  # Basic features\n",
    "                    'RSI', 'MACD', 'Signal_Line', 'Bollinger_Width',  # Technical indicators\n",
    "                    'Day_of_Week', 'Month',  # Time-based features\n",
    "                    'Lag1_Open', 'Lag2_Open', 'Lag1_Volume',\n",
    "                    'Average_sentiment', 'news_day',  # Lagged features\n",
    "                    'I-A', 'I-CT', 'I-RD', 'I-DC', 'I-DI', 'I-GI', 'I-NC',\n",
    "                    'I-RSS', 'I-SD', 'I-SR', 'I-SS', 'O', 'I-GC' # Business Event Indicator variables\n",
    "                ]\n",
    "        ].values\n",
    "\n",
    "        # Convert to PyTorch tensor\n",
    "        x = torch.tensor(features, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "        # Predict the next day's Movement probability\n",
    "        with torch.no_grad():\n",
    "            output = model(x)\n",
    "            probability = torch.sigmoid(output).item()\n",
    "            predictions.append(probability)\n",
    "\n",
    "        # Get the actual value for the next day\n",
    "        actual = test_df['Movement'].iloc[i + seq_length]\n",
    "        actuals.append(actual)\n",
    "\n",
    "        # Fine-tune the model using the current data point\n",
    "        label = torch.tensor([[actual]], dtype=torch.float32).to(device)\n",
    "\n",
    "        # Take multiple fine-tuning steps (optional)\n",
    "        for _ in range(fine_tune_steps):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x)\n",
    "            loss = criterion(output, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    return predictions, actuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 60  # Rolling window size\n",
    "fine_tune_steps = 1  # Number of gradient steps per update\n",
    "predictions, actuals = rolling_window_predictions_with_finetuning(\n",
    "    model, test_df, seq_length, device, optimizer, criterion, fine_tune_steps\n",
    ")\n",
    "\n",
    "# Convert probabilities to binary predictions\n",
    "threshold = 0.5  # Adjust threshold as needed\n",
    "predicted_classes = [1 if p > threshold else 0 for p in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5253086664814006\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      1.00      0.69     13232\n",
      "           1       0.00      0.00      0.00     11957\n",
      "\n",
      "    accuracy                           0.53     25189\n",
      "   macro avg       0.26      0.50      0.34     25189\n",
      "weighted avg       0.28      0.53      0.36     25189\n",
      "\n",
      "Confusion Matrix:\n",
      "[[13232     0]\n",
      " [11957     0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Chris\\anaconda3\\envs\\p310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Chris\\anaconda3\\envs\\p310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Chris\\anaconda3\\envs\\p310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", accuracy_score(actuals, predicted_classes))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(actuals, predicted_classes))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(actuals, predicted_classes))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
